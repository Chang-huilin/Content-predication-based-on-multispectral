{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_Block(nn.Module):                         # Squeeze-and-Excitation block\n",
    "    def __init__(self, in_channels):\n",
    "        super(SE_Block, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))     # GAP\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 16, kernel_size=1)   # 1x1的卷积核充当FC\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels // 16, in_channels, kernel_size=1)   # 1x1的卷积核充当FC\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        out = self.sigmoid(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):      # 左侧的 residual block 结构（18-layer、34-layer）\n",
    "\n",
    "    # 如果是BasicBlock，每个小block的输入=输出\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):      # 两层卷积 Conv2d + Shutcuts\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # 两个3x3卷积核\n",
    "        self.conv = nn.Sequential(\n",
    "            # 第一个cov是用来改变WxH的（stride可指定）\n",
    "            ConvBN(in_channels=in_channels, out_channels=out_channels, \n",
    "                       kernel_size=3, stride=stride, padding=1),\n",
    "            \n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 第二个conv的stride恒为1，不改变WxH\n",
    "            ConvBN(in_channels=out_channels, out_channels=out_channels, \n",
    "                   kernel_size=3, stride=1, padding=1),     \n",
    "        )\n",
    "        \n",
    "        # 新增的SE-Block\n",
    "        self.SE = SE_Block(out_channels)           # Squeeze-and-Excitation block\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        # Shortcuts用于构建 Conv Block 和 Identity Block\n",
    "        if stride != 1 or in_channels != self.expansion*out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                # 卷积+BN，不激活\n",
    "                ConvBN(in_channels=in_channels, out_channels=self.expansion*out_channels, \n",
    "                   kernel_size=1, stride=stride)\n",
    "            )\n",
    "            \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        \n",
    "        SE_out = self.SE(out)   # 经过Residual-block后再经过SE-block\n",
    "        \n",
    "        out = out * SE_out      # 对位相乘，重新加权\n",
    "        \n",
    "        out += self.shortcut(x)\n",
    "        return self.relu(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):      # 右侧的 residual block 结构（50-layer、101-layer、152-layer）\n",
    "    \n",
    "    # 观察50-layer可以发现，各个block内部卷积核通道数是4倍的关系\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):      # 三层卷积 Conv2d + Shutcuts\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        # 1x1 -> 3x3 -> 1x1\n",
    "        self.conv = nn.Sequential(\n",
    "            # 第一个cov是用来降维的，减少参数量\n",
    "            ConvBN(in_channels=in_channels, out_channels=out_channels, \n",
    "                   kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 第二个conv是用来改变WxH的（stride可指定）\n",
    "            ConvBN(in_channels=out_channels, out_channels=out_channels, \n",
    "                   kernel_size=3, stride=stride, padding=1),\n",
    "            nn.ReLU(inplace=True),  \n",
    "            \n",
    "            # 第三个conv用来升维\n",
    "            ConvBN(in_channels=out_channels, out_channels=self.expansion*out_channels, \n",
    "                   kernel_size=1)       \n",
    "        )  \n",
    "        \n",
    "        self.SE = SE_Block(self.expansion*out_channels)           # Squeeze-and-Excitation block\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        # Shortcuts用于构建 Conv Block 和 Identity Block\n",
    "        if stride != 1 or in_channels != self.expansion*out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                # 卷积+BN，不激活\n",
    "                ConvBN(in_channels=in_channels, out_channels=self.expansion*out_channels, \n",
    "                   kernel_size=1, stride=stride)\n",
    "            )\n",
    "            \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        \n",
    "        SE_out = self.SE(out)   # 经过Residual-block后再经过SE-block\n",
    "        \n",
    "        out = out * SE_out      # 对位相乘，重新加权\n",
    "        \n",
    "        out += self.shortcut(x)\n",
    "        return self.relu(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_ResNet(nn.Module):\n",
    "    def __init__(self, block, numlist_blocks, num_classes=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            block:      选用BasicBlock还是Bottleneck这两种残差结构\n",
    "            num_blocks: 针对不同数量的layers，有不同的组合，比如ResNet50为[3, 4, 6, 3]\n",
    "            num_classes:最终分类数量\n",
    "        \"\"\"\n",
    "        super(SE_ResNet, self).__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "\n",
    "        # 原始输入为229x229x3 -> 112x112x64\n",
    "        self.conv1 = ConvBN(in_channels=3, out_channels=64, kernel_size=7, stride=2) # conv1\n",
    "        # 112x112x64 -> 56x56x64\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)         # maxpool\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64,  numlist_blocks[0], stride=1)      # conv2_x\n",
    "        self.layer2 = self._make_layer(block, 128, numlist_blocks[1], stride=2)      # conv3_x\n",
    "        self.layer3 = self._make_layer(block, 256, numlist_blocks[2], stride=2)      # conv4_x\n",
    "        self.layer4 = self._make_layer(block, 512, numlist_blocks[3], stride=2)      # conv5_x\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))     # 平均池化\n",
    "        self.linear = nn.Linear(2048, num_classes)      # 线性层\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, num_blocks, stride):\n",
    "        # 虽然每个convn_x由多个block组成，但是其中只有某个block的stride为2，剩余的为1\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, in_channels, stride))\n",
    "            \n",
    "            # 经过某个convn_x之后，in_channels被放大对应expansion倍\n",
    "            self.in_channels = in_channels * block.expansion\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))    # conv1\n",
    "        x = self.maxpool(x)             # maxpool\n",
    "        x = self.layer1(x)              # conv2_x\n",
    "        x = self.layer2(x)              # conv3_x\n",
    "        x = self.layer3(x)              # conv4_x\n",
    "        x = self.layer4(x)              # conv5_x\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        out = self.linear(x)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(net, num_classes):\n",
    "    if net == 'SE_ResNet18':\n",
    "        return SE_ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "    if net == 'SE_ResNet34':\n",
    "        return SE_ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "    if net == 'SE_ResNet50':\n",
    "        return SE_ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "    if net == 'SE_ResNet101':\n",
    "        return SE_ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "    if net == 'SE_ResNet152':\n",
    "        return SE_ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n",
    "        \n",
    "def test():\n",
    "    SE_ResNet50 = make_net('SE_ResNet50', num_classes=2)\n",
    "    #创建模型，部署gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    SE_ResNet50.to(device)\n",
    "    summary(SE_ResNet50, (3, 229, 229))\n",
    "    \n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
